"""FastMCP server for tracking agent-generated code."""

import os
import sys
import argparse
from mcp.server.fastmcp import FastMCP
import httpx
import uvicorn

# Initialize FastMCP server with instructions
mcp = FastMCP(
    "AI LOC Tracker",
    instructions="""
CRITICAL RULES - YOU MUST FOLLOW THESE:

1. BEFORE generating ANY code:
   - ALWAYS call list_recent_features() first
   - Show the list to the user
   - Ask user: "Which feature are you working on? (Type existing name or new name)"

2. AFTER generating code and updating files:
   - Call track_agent_code() ONCE at the END of your response
   - This should be the LAST thing you do before ending
   - Count total LOC across all files you generated/updated
   - Count total number of files you updated
   - Use the EXACT feature_name from user's response
   - NEVER use "unknown" as feature_name

3. Workflow:
   Step 1: list_recent_features() → Show to user
   Step 2: Ask user for feature_name
   Step 3: Generate code and update files
   Step 4: Before ending response → track_agent_code(loc_count, feature_name, total_files)

4. track_agent_code parameters:
   - loc_count: Total LOC across all files you generated
   - feature_name: From user (exact match if updating existing)
   - total_files: Number of files you updated
   - code_type: "code", "test", or "documentation"

5. ONLY 2 TOOLS:
   - list_recent_features(): Call at the start
   - track_agent_code(): Call ONCE at the end before finishing response
""",
)

# Get backend URL from environment or use default
BACKEND_URL = os.getenv("METRICS_BACKEND_URL", "http://localhost:8000")
http_client = httpx.AsyncClient(timeout=10.0)


@mcp.tool()
def track_agent_code(
    loc_count: int,
    feature_name: str,
    total_files: int = 1,
    code_type: str = "code",
    developer_id: str = "unknown",
) -> dict:
    """
    Track code generated by AI agent - CALL THIS ONCE BEFORE ENDING RESPONSE.

    ⚠️ CRITICAL RULES:
    1. ALWAYS call list_recent_features() FIRST to see existing features
    2. Show features to user and ask for feature_name
    3. Call this tool ONCE at the END of your response (after all file updates)
    4. NEVER use "unknown" as feature_name

    Args:
        loc_count: Total number of lines of code (LOC) generated - REQUIRED
        feature_name: Name/description of the feature - REQUIRED (ask user, never use "unknown")
        total_files: Number of files that were updated (default: 1)
        code_type: Type of code - "code", "test", or "documentation" (default: "code")
        developer_id: Developer identifier (default: "unknown")

    Returns:
        Success response with lines tracked
    """
    if loc_count <= 0:
        return {"success": False, "message": "LOC count must be greater than 0"}

    if total_files <= 0:
        return {"success": False, "message": "total_files must be greater than 0"}

    if not feature_name or feature_name.strip().lower() == "unknown":
        return {
            "success": False,
            "message": "feature_name is REQUIRED and cannot be 'unknown'. Please call list_recent_features() first, then ask user for feature name.",
        }

    # Determine event endpoint based on code_type
    endpoint_map = {
        "code": "/api/events/code",
        "test": "/api/events/test",
        "documentation": "/api/events/documentation",
    }

    endpoint = endpoint_map.get(code_type, "/api/events/code")

    # Prepare event payload
    event = {
        "source": "agent",
        "lines": loc_count,
        "file_path": f"{total_files} file(s)",  # Store file count instead of path
        "language": "unknown",
        "developer_id": developer_id,
        "type": code_type,
        "metadata": {
            "feature_name": feature_name,
            "total_files": total_files,
        },
    }

    # Add type-specific fields
    if code_type == "test":
        event["test_framework"] = "unknown"
    elif code_type == "documentation":
        event["doc_type"] = "api-doc"

    # Send to backend
    try:
        response = httpx.post(f"{BACKEND_URL}{endpoint}", json=event, timeout=5.0)
        response.raise_for_status()
        return {
            "success": True,
            "message": f"Agent code tracked: {loc_count} LOC across {total_files} file(s)",
            "loc_count": loc_count,
            "feature_name": feature_name,
            "total_files": total_files,
            "code_type": code_type,
        }
    except httpx.RequestError as e:
        return {
            "success": False,
            "message": f"Failed to send event to backend: {str(e)}",
        }
    except httpx.HTTPStatusError as e:
        return {
            "success": False,
            "message": f"Backend error: {e.response.status_code}",
        }


@mcp.resource("metrics://developer/{developer_id}")
def get_developer_metrics(developer_id: str) -> str:
    """
    Get metrics for a specific developer.

    Args:
        developer_id: Developer identifier

    Returns:
        JSON string with developer metrics
    """
    try:
        response = httpx.get(
            f"{BACKEND_URL}/api/metrics/developer/{developer_id}",
            timeout=5.0,
        )
        response.raise_for_status()
        return response.text
    except httpx.RequestError as e:
        return f'{{"error": "Failed to get metrics: {str(e)}"}}'
    except httpx.HTTPStatusError as e:
        return f'{{"error": "Backend error: {e.response.status_code}"}}'


@mcp.resource("metrics://team")
def get_team_metrics() -> str:
    """
    Get team-wide metrics and leaderboard.

    Returns:
        JSON string with team metrics
    """
    try:
        response = httpx.get(f"{BACKEND_URL}/api/metrics/team", timeout=5.0)
        response.raise_for_status()
        return response.text
    except httpx.RequestError as e:
        return f'{{"error": "Failed to get metrics: {str(e)}"}}'
    except httpx.HTTPStatusError as e:
        return f'{{"error": "Backend error: {e.response.status_code}"}}'


@mcp.tool()
def list_recent_features(limit: int = 20) -> dict:
    """
    List recent features with their total LOC counts.

    ⚠️ CALL THIS FIRST before generating any code!
    Show the results to user and ask which feature they're working on.

    Args:
        limit: Maximum number of features to return (default: 20)

    Returns:
        Dictionary with recent features and their LOC counts
    """
    try:
        response = httpx.get(
            f"{BACKEND_URL}/api/metrics/features",
            params={"limit": limit},
            timeout=5.0,
        )
        response.raise_for_status()
        return response.json()
    except httpx.RequestError as e:
        return {
            "success": False,
            "error": f"Failed to get features: {str(e)}",
            "features": [],
        }
    except httpx.HTTPStatusError as e:
        return {
            "success": False,
            "error": f"Backend error: {e.response.status_code}",
            "features": [],
        }


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="MCP Server for AI LOC Tracker")
    parser.add_argument(
        "--transport",
        type=str,
        default="stdio",
        choices=["stdio", "sse"],
        help="Transport type: stdio or sse (default: stdio)",
    )
    parser.add_argument(
        "--port",
        type=int,
        default=8001,
        help="Port number for SSE transport (default: 8001)",
    )
    parser.add_argument(
        "--host",
        type=str,
        default="localhost",
        help="Host for SSE transport (default: localhost)",
    )

    args = parser.parse_args()

    if args.transport == "sse":
        # Run with SSE transport on specified port
        print(f"Starting MCP server on {args.host}:{args.port} with SSE transport...")
        print(f"Backend URL: {BACKEND_URL}")
        print(f"Connect Cursor to: http://{args.host}:{args.port}")

        # FastMCP with SSE transport
        # Note: FastMCP may need uvicorn for SSE, so we'll use it directly
        try:
            # Try FastMCP's built-in SSE support if available
            mcp.run(transport="sse", host=args.host, port=args.port)
        except (TypeError, ValueError) as e:
            # If FastMCP doesn't support host/port directly,
            # we need to use uvicorn with FastAPI
            print(f"Using uvicorn to run server (FastMCP limitation: {e})")
            from fastapi import FastAPI
            from fastapi.responses import StreamingResponse
            import json
            import asyncio

            app = FastAPI(title="MCP Server - AI LOC Tracker")

            # Create SSE endpoint for MCP
            @app.get("/sse")
            async def sse_endpoint():
                async def event_generator():
                    # Send initial connection message
                    yield f"data: {json.dumps({'type': 'connected'})}\n\n"
                    # Keep connection alive
                    while True:
                        await asyncio.sleep(30)
                        yield f"data: {json.dumps({'type': 'ping'})}\n\n"

                return StreamingResponse(
                    event_generator(),
                    media_type="text/event-stream",
                    headers={
                        "Cache-Control": "no-cache",
                        "Connection": "keep-alive",
                    },
                )

            @app.post("/messages")
            async def handle_mcp_message(message: dict):
                # Handle MCP protocol messages
                return {"status": "ok", "message": "MCP message received"}

            uvicorn.run(app, host=args.host, port=args.port, log_level="info")
    else:
        # Run with stdio transport for Cursor/Windsurf
        mcp.run(transport="stdio")
